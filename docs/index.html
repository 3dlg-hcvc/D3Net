<!DOCTYPE html>
<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>D3Net: A Speaker-Listener Architecture for Semi-supervised Dense Captioning and Visual Grounding in RGB-D Scans</title>
    <link rel="stylesheet" href="w3.css">
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js" integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI" crossorigin="anonymous"></script>
</head>

<body>

<br/>
<br/>

<div class="w3-container" id="paper">
    <div class="w3-content" style="max-width:850px">
  
    <h2 align="center" id="title"><b>D3Net: A Speaker-Listener Architecture for Semi-supervised Dense Captioning and Visual Grounding in RGB-D Scans</b></h2>
    <br/>

    <p align="center" class="center_text" id="authors">
        <a target="_blank" href="https://daveredrum.github.io/">Dave Zhenyu Chen</a><sup>1</sup>
        &nbsp;&nbsp;&nbsp;&nbsp;

        <a target="_blank" href="">Qirui Wu</a><sup>2</sup>
        &nbsp;&nbsp;&nbsp;&nbsp;
        
        <a target="_blank" href="https://www.niessnerlab.org/members/matthias_niessner/profile.html">Matthias Nie&szlig;ner</a><sup>1</sup>
        &nbsp;&nbsp;&nbsp;&nbsp;

        <a target="_blank" href="https://angelxuanchang.github.io/">Angel X. Chang</a><sup>2</sup>
        &nbsp;&nbsp;&nbsp;&nbsp;
    </p>

    <p class="center_text" align="center">
        <sup>1</sup>Technical University of Munich
        &nbsp; &nbsp; &nbsp;
        <sup>2</sup>Simon Fraser University
    </p>

    <br><center><img src="teaser.gif" style="width:100%" /></center><br>

    <h3 class="w3-left-align" id="video"><b>Introduction</b></h3>
    <p>
        Recent studies on dense captioning and visual grounding in 3D have achieved impressive results. Despite developments in both areas, the limited amount of available 3D vision-language data causes overfitting issues for 3D visual grounding and 3D dense captioning methods. Also, how to discriminatively describe objects in complex 3D environments is not fully studied yet. To address these challenges, we present D3Net, an end-to-end neural speaker-listener architecture that can detect, describe and discriminate. Our D3Net unifies dense captioning and visual grounding in 3D in a self-critical manner. This self-critical property of D3Net also introduces discriminability during object caption generation and enables semi-supervised training on ScanNet data with partially annotated descriptions. Our method outperforms SOTA methods in both tasks on the ScanRefer dataset, surpassing the SOTA 3D dense captioning method by a significant margin (23.56% CiDEr@0.5IoU improvement).
    </p>

    <h3 class="w3-left-align" id="video"><b>Video</b></h3>
    <p>
    <iframe width="850" height="480" src="https://www.youtube.com/embed/mIPNzoVOGN4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    <p/>

    <h3 class="w3-left-align" id="video"><b>Results</b></h3>
    <p>
    <center><img src="captioning.png" style="width:100%" /></center>
    <br/>
    Qualitative results in 3D dense captioning task from <span style="background-color: #ffe699; font-weight: bold;">Scan2Cap</span> and <span style="background-color: #bdd7ee; font-weight: bold;">our method</span>. We underline the inaccurate words and mark the spatially discriminative phrases in bold. <span style="background-color: #bdd7ee; font-weight: bold;">Our method</span> qualitatively outperforms <span style="background-color: #ffe699; font-weight: bold;">Scan2Cap</span> in producing better object bounding boxes and more discriminative descriptions.
    <p/>
    <br/>

    <p>
    <center><img src="grounding.png" style="width:100%" /></center>
    <br/>
    3D visual grounding results using <span style="background-color: #f0dada; font-weight: bold;">3DVG-Transformer</span> and <span style="background-color: #deebf7; font-weight: bold;">our method</span>. <span style="background-color: #f0dada; font-weight: bold;">3DVG-Transformer</span> fails to accurately predict object bounding boxes, while <span style="background-color: #deebf7; font-weight: bold;">our method</span> produces accurate bounding boxes and correctly distinguishes target objects from distractors.
    <p/>
    <br/>

    <h3 class="w3-left-align" id="publication"><b>Publication</b></h3>
    <a href="D3Net.pdf" target="__blank">Paper</a> | <a href="https://arxiv.org/abs/2112.01551" target="__blank">arXiv</a> | <a href="https://github.com/daveredrum/D3Net" target="__blank">Code</a>
    <center>
        <a href="D3Net.pdf" target="__blank"><img src="paper.jpg" style="max-width:100%" /></a>
    </center><br>

    If you find our project useful, please consider citing us:
    <pre class="w3-panel w3-leftbar w3-light-grey" style="white-space: pre-wrap; font-family: monospace; font-size: 11px">

@misc{chen2021d3net,
    title={D3Net: A Speaker-Listener Architecture for Semi-supervised Dense Captioning and Visual Grounding in RGB-D Scans}, 
    author={Dave Zhenyu Chen and Qirui Wu and Matthias Nie√üner and Angel X. Chang},
    year={2021},
    eprint={2112.01551},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

</pre>

    </div>


</div>

<br/>
<br/>

</body>
</html>